{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Review) Multinomial Classification\n",
    "\n",
    "### Softmax Function (Hypothesis)\n",
    "\n",
    "$$\n",
    "S(y_{i}) = {e^{y_i} \\over \\sum_{j=1}^n e^{y_{j}}} \\\\\n",
    "n: Number\\ of\\ classes \\\\\n",
    "\\\\\n",
    "i: i\\ class\n",
    "$$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "$$\n",
    "Cost(W) = {1 \\over m} {\\sum_{i=1}^m D(S(X_{i}W + b),L_{i})} \\\\\n",
    "\\\\\n",
    "m: Number\\ of\\ instances \\\\\n",
    "\\\\\n",
    "i: i\\ instance\n",
    "$$\n",
    "\n",
    "### Cross Entropy in Multinomial Classification\n",
    "\n",
    "$$\n",
    "D(S, L) = - \\sum_{j=1}^n L_{j} log(S(y_{j})) \\\\\n",
    "\\\\\n",
    "n: Number\\ of\\ classes \\\\\n",
    "\\\\\n",
    "j: j\\ class\n",
    "$$\n",
    "\n",
    "### Minimizing Cost\n",
    "\n",
    "$$\n",
    "W_{new} = W_{old} - \\alpha {\\partial \\over {\\partial W}} Cost(W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "print(\"TensorFlow Version: %s\" % (tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_iris()\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data: (150, 4)\n",
      "y_data: (150,)\n",
      "nb_features: 4\n",
      "nb_classes: 3\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array(df.data, dtype=np.float32)\n",
    "y_data = np.array(df.target, dtype=np.int32)\n",
    "\n",
    "nb_features = x_data.shape[1]\n",
    "nb_classes = len(set(y_data))\n",
    "\n",
    "print(\"x_data:\", x_data.shape)\n",
    "print(\"y_data:\", y_data.shape)\n",
    "\n",
    "print(\"nb_features:\", nb_features)\n",
    "print(\"nb_classes:\", nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Weights: \n",
      " [[-0.10099822  0.6847899   1.6258513 ]\n",
      " [ 0.88112587 -0.63692456 -0.1427695 ]\n",
      " [ 0.82411087 -0.91326994 -0.4510184 ]\n",
      " [ 0.58053356  1.3066356  -0.60428965]] \n",
      "\n",
      "# Bias: \n",
      " [ 0.38414612 -0.6159301  -0.5453214 ]\n"
     ]
    }
   ],
   "source": [
    "# Weights\n",
    "tf.random.set_seed(2020)\n",
    "W = tf.Variable(tf.random.normal([nb_features, nb_classes], mean=0.0))\n",
    "b = tf.Variable(tf.random.normal([nb_classes], mean=0.0))\n",
    "\n",
    "# One-Hot Encoding\n",
    "y_one_hot = tf.one_hot(indices=list(y_data), depth=nb_classes)\n",
    "\n",
    "print('# Weights: \\n', W.numpy(), '\\n\\n# Bias: \\n', b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> #0 \n",
      " Weights: \n",
      "[[-0.11743642  0.70453906  1.6225402 ]\n",
      " [ 0.87678254 -0.62771267 -0.14763813]\n",
      " [ 0.80348897 -0.8990887  -0.44457778]\n",
      " [ 0.5725719   1.3110503  -0.60074264]] \n",
      " Bias: \n",
      "[ 0.38213396 -0.61260384 -0.54663545] \n",
      " cost: 3.9675086\n",
      "\n",
      ">>> #1000 \n",
      " Weights: \n",
      "[[ 0.16433352  1.3574841   0.68781847]\n",
      " [ 1.7573066  -0.6487719  -1.0071023 ]\n",
      " [-0.8606213  -0.5406396   0.8610837 ]\n",
      " [-0.19550529  1.0364839   0.44190007]] \n",
      " Bias: \n",
      "[ 0.5655053  -0.43386927 -0.9087408 ] \n",
      " cost: 0.40306073\n",
      "\n",
      ">>> #2000 \n",
      " Weights: \n",
      "[[ 0.31688696  1.5321444   0.3606074 ]\n",
      " [ 2.0306034  -0.699498   -1.2296748 ]\n",
      " [-1.2645828  -0.59373426  1.3181392 ]\n",
      " [-0.38164774  0.7259959   0.9385294 ]] \n",
      " Bias: \n",
      "[ 0.6280675  -0.33476618 -1.0704058 ] \n",
      " cost: 0.29343706\n",
      "\n",
      ">>> #3000 \n",
      " Weights: \n",
      "[[ 0.42719242  1.6423255   0.14011964]\n",
      " [ 2.222088   -0.7123837  -1.4082739 ]\n",
      " [-1.5467921  -0.6306485   1.6372662 ]\n",
      " [-0.51172817  0.4970247   1.297582  ]] \n",
      " Bias: \n",
      "[ 0.6729265 -0.2532307 -1.1967993] \n",
      " cost: 0.23829417\n",
      "\n",
      ">>> #4000 \n",
      " Weights: \n",
      "[[ 0.5144236   1.717074   -0.02186037]\n",
      " [ 2.3705711  -0.7091585  -1.5599803 ]\n",
      " [-1.7671052  -0.6548329   1.8817648 ]\n",
      " [-0.6135871   0.31790695  1.5785577 ]] \n",
      " Bias: \n",
      "[ 0.7084192  -0.18307595 -1.302447  ] \n",
      " cost: 0.20493008\n",
      "\n",
      ">>> #5000 \n",
      " Weights: \n",
      "[[ 0.5868196   1.7708837  -0.14806792]\n",
      " [ 2.491839   -0.6979823  -1.692425  ]\n",
      " [-1.9482718  -0.67131     2.0794072 ]\n",
      " [-0.697666    0.17082885  1.809714  ]] \n",
      " Bias: \n",
      "[ 0.73791    -0.12065024 -1.3943636 ] \n",
      " cost: 0.18246923\n",
      "\n",
      ">>> #6000 \n",
      " Weights: \n",
      "[[ 0.648823    1.8113612  -0.25055158]\n",
      " [ 2.5943112  -0.6827013  -1.8101772 ]\n",
      " [-2.102253   -0.6829624   2.2450397 ]\n",
      " [-0.7694064   0.04584497  2.0064383 ]] \n",
      " Bias: \n",
      "[ 0.76319474 -0.06381484 -1.476484  ] \n",
      " cost: 0.16624019\n",
      "\n",
      ">>> #7000 \n",
      " Weights: \n",
      "[[ 0.7031186   1.842829   -0.33631983]\n",
      " [ 2.683033   -0.6653318  -1.9162688 ]\n",
      " [-2.2362247  -0.6914313   2.3874798 ]\n",
      " [-0.83205914 -0.06300665  2.177942  ]] \n",
      " Bias: \n",
      "[ 0.78535825 -0.01123109 -1.5512308 ] \n",
      " cost: 0.1539142\n",
      "\n",
      ">>> #8000 \n",
      " Weights: \n",
      "[[ 0.7514567   1.8679215  -0.40975812]\n",
      " [ 2.7612686  -0.6469993  -2.012836  ]\n",
      " [-2.3548448  -0.6977029   2.5123684 ]\n",
      " [-0.8877303  -0.15955041  2.3301554 ]] \n",
      " Bias: \n",
      "[ 0.8051083   0.03799404 -1.6202061 ] \n",
      " cost: 0.14420128\n",
      "\n",
      ">>> #9000 \n",
      " Weights: \n",
      "[[ 0.7950464   1.8883388  -0.4737722 ]\n",
      " [ 2.8312438  -0.628357   -2.1014526 ]\n",
      " [-2.4613101  -0.7024015   2.6235294 ]\n",
      " [-0.93786424 -0.24637954  2.467118  ]] \n",
      " Bias: \n",
      "[ 0.8229338  0.0844864 -1.684524 ] \n",
      " cost: 0.13632807\n",
      "\n",
      ">>> #10000 \n",
      " Weights: \n",
      "[[ 0.8347582  1.9052253 -0.5303759]\n",
      " [ 2.8945463 -0.6097894 -2.183319 ]\n",
      " [-2.5579088 -0.7059436  2.7236636]\n",
      " [-0.9834962 -0.3253334  2.591702 ]] \n",
      " Bias: \n",
      "[ 0.83918715  0.12870272 -1.7449929 ] \n",
      " cost: 0.12980223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Softmax Function\n",
    "def softmax(X):\n",
    "    sm = tf.nn.softmax(tf.matmul(x_data, W) + b)\n",
    "    return sm\n",
    "\n",
    "# Training\n",
    "for i in range(10000+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        sm = softmax(x_data)\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y_one_hot*tf.math.log(sm), axis=1))        \n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "                \n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\">>> #%s \\n Weights: \\n%s \\n Bias: \\n%s \\n cost: %s\\n\" % (i, W.numpy(), b.numpy(), cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97333336\n"
     ]
    }
   ],
   "source": [
    "predicted = tf.argmax(softmax(x_data), axis=1)\n",
    "real = tf.argmax(y_one_hot, axis=1)\n",
    "\n",
    "def acc(predicted, real):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, real), dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "accuracy = acc(predicted, real).numpy()\n",
    "print(\"Accuracy: %s\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.nn.softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Weights: \n",
      " [[-0.10099822  0.6847899   1.6258513 ]\n",
      " [ 0.88112587 -0.63692456 -0.1427695 ]\n",
      " [ 0.82411087 -0.91326994 -0.4510184 ]\n",
      " [ 0.58053356  1.3066356  -0.60428965]] \n",
      "\n",
      "# Bias: \n",
      " [ 0.38414612 -0.6159301  -0.5453214 ]\n"
     ]
    }
   ],
   "source": [
    "# Weights\n",
    "tf.random.set_seed(2020)\n",
    "W = tf.Variable(tf.random.normal([nb_features, nb_classes], mean=0.0), name='Weights')\n",
    "b = tf.Variable(tf.random.normal([nb_classes], mean=0.0), name='Bias')\n",
    "variables=[W, b]\n",
    "\n",
    "# One-Hot Encoding\n",
    "y_one_hot = tf.one_hot(indices=list(y_data), depth=nb_classes)\n",
    "\n",
    "print('# Weights: \\n', W.numpy(), '\\n\\n# Bias: \\n', b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_fn(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "\n",
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y) # Y: One-Hot Encoded\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    return cost\n",
    "\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "    return grads\n",
    "\n",
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def fit(X, Y, epochs=10000, verbose=1000, learning_rate=0.01):\n",
    "    for i in range(epochs+1):\n",
    "        grads = grad_fn(X, Y)\n",
    "        variables[0].assign_sub(learning_rate * grads[0])\n",
    "        variables[1].assign_sub(learning_rate * grads[1])\n",
    "        \n",
    "        if i % verbose == 0:\n",
    "            acc = prediction(X, Y).numpy()\n",
    "            loss = cost_fn(X, Y).numpy()\n",
    "            print(\"Epoch: {} \\t Loss: {} \\t Acc: {}\".format(i, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 3.812716007232666 \t Acc: 0.0\n",
      "Epoch: 1000 \t Loss: 0.4028949439525604 \t Acc: 0.9133333563804626\n",
      "Epoch: 2000 \t Loss: 0.2933638393878937 \t Acc: 0.95333331823349\n",
      "Epoch: 3000 \t Loss: 0.2382526695728302 \t Acc: 0.9666666388511658\n",
      "Epoch: 4000 \t Loss: 0.20490330457687378 \t Acc: 0.9733333587646484\n",
      "Epoch: 5000 \t Loss: 0.18245039880275726 \t Acc: 0.9733333587646484\n",
      "Epoch: 6000 \t Loss: 0.16622616350650787 \t Acc: 0.9733333587646484\n",
      "Epoch: 7000 \t Loss: 0.15390338003635406 \t Acc: 0.9733333587646484\n",
      "Epoch: 8000 \t Loss: 0.1441926211118698 \t Acc: 0.9733333587646484\n",
      "Epoch: 9000 \t Loss: 0.13632096350193024 \t Acc: 0.9733333587646484\n",
      "Epoch: 10000 \t Loss: 0.12979625165462494 \t Acc: 0.9733333587646484\n"
     ]
    }
   ],
   "source": [
    "fit(x_data, y_one_hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
